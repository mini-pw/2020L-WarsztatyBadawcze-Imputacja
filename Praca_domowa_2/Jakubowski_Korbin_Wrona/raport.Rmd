---
title: "Imputation methods' comparison"
author: "Miko≈Çaj Jakubowski, Marceli Korbin, Patryk Wrona"
date: "27th of April, 2020"
output: html_document
---

```{r setup, include=FALSE}
library(VIM)
library(mice)
library(missForest)
library(dplyr)
library(microbenchmark)
library(OpenML)
knitr::opts_chunk$set(cache = TRUE)
```

## Objective

In the report, we will evaluate machine learning algorithms' performance based on the completion of datasets and several data imputation techniques.

## Methods

We are going to implement (using R language) and evaluate five imputation methods:  

* deleting features that contain any missing data;  
* completing columns with mean (numeric features), median (integer features) or dominant (factor features);  
* IRMI method from the VIM package;  
* mice imputation;  
* missForest imputation.

The first two are basic, manual methods, while the next three come from R packages designed for and focusing on working with data imputation. With such set, we give ourselves the opportunity to check the performance basing on the complexity of methods.

```{r imputing}
library(VIM)
library(mice)
library(missForest)
library(dplyr)

imputation <- function(df, num){
  # 1: removing columns with missing data
  # 2: imputing with: mean (continous variables),
  # median (ordinal variables)
  # modal value (nominal variables)
  # 3: IRMI from VIM
  # 4: mice
  # 5: missForest
  
  if (num==1)
    return(df[, colSums(is.na(df))==0])
  
  else if (num==2){
    classes <- sapply(df[, -ncol(df)], class)
    moda <- function(x){
      u <- unique(x)
      u <- u[!is.na(u)]
      u[which.max(tabulate(match(x, u)))]
    }
    nas <- colSums(is.na(df[, -ncol(df)]))
    for (i in 1:(ncol(df)-1)) if (nas[[i]]>0){
      if(classes[[i]]=="numeric")
        df[is.na(df[,i]), i] <- mean(df[[i]], na.rm=T)
      else if(classes[[i]]=="integer")
        df[is.na(df[,i]), i] <- median(df[[i]], na.rm=T)
      else
        df[is.na(df[,i]), i] <- moda(df[[i]])
    }
    return(df)
  }
  
  else if (num==3){
    fit <- VIM::irmi(df[, -ncol(df)], trace=F, imp_var=F, maxit=10) %>%
      cbind.data.frame(df[, ncol(df)])
    colnames(fit)[ncol(fit)] <- colnames(df)[ncol(df)]
    return(fit)
  }
  
  else if (num==4){
    fit <- mice(df[, -ncol(df)], m=2, maxit=3, printFlag=F)
    comp <- complete(fit) %>%
      cbind.data.frame(df[, ncol(df)])
    colnames(comp)[ncol(comp)] <- colnames(df)[ncol(df)]
    return(comp)
  }
  
  else if (num==5){
    fit <- missForest(df[, -ncol(df)], maxiter=3, ntree=15)
    comp <- fit$ximp %>%
      cbind.data.frame(df[, ncol(df)])
    colnames(comp)[ncol(comp)] <- colnames(df)[ncol(df)]
    return(comp)
  }
}
```

The methods will be compared through measuring the evaluation time. We make the measurement by using the microbenchmark function. Number of tests shall vary depending on the dataset, since its size happens to greatly affect the time of a single test irrespectively of the method.

```{r measure}
# source('./imputing_function.R')

measureImputationTime <- function(df, number_of_tests){
  measured <- microbenchmark::microbenchmark(
    removing = imputation(df, 1),
    mean_med_modal = imputation(df, 2),
    IRMI_VIM = imputation(df, 3),
    mice = imputation(df, 4),
    missForest = imputation(df, 5),
    times = number_of_tests
  )
  mean_times <- measured %>%
                group_by(expr) %>%
                summarize(mean_time = mean(time))
  return(mean_times)
}
```

As for the next step, we will train a machine learning algorithm on each of the imputation outputs for every dataset; this gives 40 trainings overall. We are testing a total of three algorithms: the gradient boosting algorithm, the random forest classifier and the logistic regression responses. The choice between them varies depending on the dataset.

### Loading data

Our 5 techniques are carried out on 8 datasets. All originate from the OpenML package and contain several missing values. Most of them, if not all, have already been analysed in previous works on the course. Their indexes are:

4, 27, 29, 38, 55, 56, 188, 944

Now we will load all above datasets as a list of data frames.

```{r, echo=FALSE, results='hide', message = FALSE, warning = FALSE}

source("./loading_data.R")
listOfDatasets <- loadAllSets()
```


### Imputation time comparison



```{r, message = FALSE, warning = FALSE}
# DATASET 4

measureImputationTime(listOfDatasets[[1]][[1]], number_of_tests = 5)
```

```{r, message = FALSE, warning = FALSE}
# DATASET 27

measureImputationTime(listOfDatasets[[2]][[1]], number_of_tests = 3)
```

```{r, message = FALSE, warning = FALSE}
# DATASET 29

measureImputationTime(listOfDatasets[[3]][[1]], number_of_tests = 3)
```

```{r, message = FALSE, warning = FALSE}
# DATASET 38

measureImputationTime(listOfDatasets[[4]][[1]], number_of_tests = 1)
```

```{r, message = FALSE, warning = FALSE}

# DATASET 55

measureImputationTime(listOfDatasets[[5]][[1]], number_of_tests = 5)
```

```{r, message = FALSE, warning = FALSE}

# DATASET 56

measureImputationTime(listOfDatasets[[6]][[1]], number_of_tests = 1)
```

```{r, message = FALSE, warning = FALSE}

# DATASET 188

measureImputationTime(listOfDatasets[[7]][[1]], number_of_tests = 5)
```

```{r, message = FALSE, warning = FALSE}

# DATASET 944

measureImputationTime(listOfDatasets[[8]][[1]], number_of_tests = 5)
```


## Machine learning models

Now, it is high time to evaluate the change of model performance across all data sets in function of used imputation algorithm:

* 1 = deleting features that contain any missing data;  
* 2 = completing columns with mean (numeric features), median (integer features) or dominant (factor features);  
* 3 = IRMI method from the VIM package;  
* 4 = mice imputation;  
* 5 = missForest imputation.

### Loading algorithms:

In order to assess imputation algorithms, we will compare the metrics of the below 3 machine learning models in function of used imputation:

- Random Forest
- Linear Regression
- Gradient Boosting

Loading these algorithms from our scripts:


```{r, echo=FALSE, results='hide', message = FALSE, warning = FALSE}

source('./gradient_boost_train_and_result.R')
source('./logistic_regression_train_and_result.R')
source('./random_forest_train_and_result.R')
```


### 1 - Random Forest

```{r, message = FALSE, warning = FALSE}
results <- list(NA)

# DATASET 4:
df <- listOfDatasets[[1]][[1]]
target <- listOfDatasets[[1]][[2]]
results[[1]] <- data.frame(acc = NA, f1 = NA, prec = NA, rec = NA)
results[[1]] <- rbind(results[[1]], data.frame(acc = NA, f1 = NA, prec = NA, rec = NA))
for(i in 3:5){
  imp <- imputation(df, i)
  aux <- getRFResults(imp, target)
  results[[1]] <- rbind(results[[1]], aux)
}

results[[1]]
```

### 2 - Linear regression


```{r, message = FALSE, warning = FALSE}
# DATASET 27:
df <- listOfDatasets[[2]][[1]]
target <- listOfDatasets[[2]][[2]]
imp <- imputation(df, 1)
results[[2]] <- getLRResults(imp, target)
for(i in 2:5){
  imp <- imputation(df, i)
  aux <- getLRResults(imp, target)
  results[[2]] <- rbind(results[[2]], aux)
}

results[[2]]
```

### 3 - Random Forest

```{r, message = FALSE, warning = FALSE}
# DATASET 29:
df <- listOfDatasets[[3]][[1]]
target <- listOfDatasets[[3]][[2]]
imp <- imputation(df, 1)
results[[3]] <- getRFResults(imp, target)
imp <- imputation(df, 2)
results[[3]] <- rbind(results[[3]], getRFResults(imp, target))
imp <- imputation(df, 3)
results[[3]] <- rbind(results[[3]], getRFResults(imp, target))
results[[3]] <- rbind(results[[3]], NA)
imp <- imputation(df, 5)
results[[3]] <- rbind(results[[3]], getRFResults(imp, target))

results[[3]]
```

### 4 - Gradient Boosting

```{r, message = FALSE, warning = FALSE}
# DATASET 38:
df <- listOfDatasets[[4]][[1]]
target <- listOfDatasets[[4]][[2]]
imp <- imputation(df, 1)
results[[4]] <- getGDBRestults(imp, target)
for(i in 2:5){
  imp <- imputation(df, i)
  aux <- getGDBRestults(imp, target)
  results[[4]] <- rbind(results[[4]], aux)
}

results[[4]]
```

### 5 - Linear regression

```{r, message = FALSE, warning = FALSE}
# DATASET 55:
df <- listOfDatasets[[5]][[1]]
target <- listOfDatasets[[5]][[2]]
imp <- imputation(df, 1)
results[[5]] <- getLRResults(imp, target)
for(i in 2:5){
  imp <- imputation(df, i)
  aux <- getLRResults(imp, target)
  results[[5]] <- rbind(results[[5]], aux)
}

results[[5]]
```

### 6 - Linear regression

```{r, message = FALSE, warning = FALSE}
# DATASET 56:
df <- listOfDatasets[[6]][[1]]
target <- listOfDatasets[[6]][[2]]
imp <- imputation(df, 1)
results[[6]] <- data.frame(acc = NA, f1 = NA, prec = NA, rec = NA)
results[[6]] <- rbind(results[[6]], data.frame(acc = NA, f1 = NA, prec = NA, rec = NA))
results[[6]] <- rbind(results[[6]], data.frame(acc = NA, f1 = NA, prec = NA, rec = NA))

for(i in 4:5){
  imp <- imputation(df, i)
  aux <- getLRResults(imp, target)
  results[[6]] <- rbind(results[[6]], aux)
}

results[[6]]
```

### 7 - Gradient Boosting

```{r, message = FALSE, warning = FALSE}
# DATASET 188:
df <- listOfDatasets[[7]][[1]]
target <- listOfDatasets[[7]][[2]]
imp <- imputation(df, 1)
results[[7]] <- getGDBRestults(imp, target)
for(i in 2:5){
  imp <- imputation(df, i)
  aux <- getGDBRestults(imp, target)
  results[[7]] <- rbind(results[[7]], aux)
}

results[[7]]
```

### 8 - Random Forest

```{r, message = FALSE, warning = FALSE}
# DATASET 944:
df <- listOfDatasets[[8]][[1]]
target <- listOfDatasets[[8]][[2]]
results[[8]] <- data.frame(acc = NA, f1 = NA, prec = NA, rec = NA)
results[[8]] <- rbind(results[[8]], data.frame(acc = NA, f1 = NA, prec = NA, rec = NA))
for(i in 3:5){
  imp <- imputation(df, i)
  aux <- getRFResults(imp, target)
  results[[8]] <- rbind(results[[8]], aux)
}

results[[8]]

```




## Results

Thanks to our results, we can deduce the best techniques of imputation that appeared in each dataset of our research:

Datasets:

1) - 3,5
2) - 3,2
3) - 5,3
4) - 5,2
5) - 5,4
6) - 5,4
7) - 3,4
8) - 3,5

Looking simply on the best imputations is not enough because there would be 2 winners. Nevertheless, judging by two best imputation techniques, one can deduce that the most occurring was number **5** that is **missForest** imputation. On the other hand, the worst imputation technique was **1 - deleting columns with NA**

